#!/bin/bash

# Task 10: Optimization and Scaling Script
# Implements performance optimization and horizontal scaling capabilities

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

echo "📈 Secretly Optimization and Scaling"
echo "===================================="

# Create optimization directories
log_info "Setting up optimization infrastructure..."
mkdir -p optimization/{performance,scaling,monitoring,caching}

# Create performance optimization configuration
log_info "Creating performance optimization settings..."
cat > optimization/performance/performance-config.yaml << 'EOF'
# Performance Optimization Configuration

# Database Optimization
database:
  connection_pool:
    max_connections: 100
    min_connections: 10
    max_idle_time: "30m"
    max_lifetime: "1h"
  
  query_optimization:
    enable_prepared_statements: true
    query_timeout: "30s"
    slow_query_threshold: "1s"
    enable_query_cache: true
  
  indexing:
    auto_analyze: true
    maintenance_work_mem: "256MB"
    effective_cache_size: "1GB"

# Application Performance
application:
  server:
    read_timeout: "30s"
    write_timeout: "30s"
    idle_timeout: "60s"
    max_header_bytes: 1048576
  
  concurrency:
    max_goroutines: 1000
    worker_pool_size: 50
    queue_buffer_size: 1000
  
  caching:
    enable_memory_cache: true
    cache_size: "512MB"
    cache_ttl: "1h"
    enable_redis_cache: true

# Web Performance
web:
  compression:
    enable_gzip: true
    compression_level: 6
    min_length: 1024
  
  static_assets:
    enable_caching: true
    cache_max_age: "1y"
    enable_etag: true
    enable_brotli: true
  
  cdn:
    enable_cdn: false
    cdn_url: ""
    cache_control: "public, max-age=31536000"

# Security Performance
security:
  rate_limiting:
    enable_adaptive_limiting: true
    burst_multiplier: 2
    recovery_time: "1m"
  
  encryption:
    enable_hardware_acceleration: true
    cipher_suite_optimization: true
    session_cache_size: 1000

# Monitoring and Metrics
monitoring:
  enable_detailed_metrics: true
  metrics_interval: "15s"
  enable_profiling: false
  profile_rate: 0.1
EOF

# Create horizontal scaling configuration
log_info "Setting up horizontal scaling configuration..."
cat > optimization/scaling/docker-compose.scaled.yml << 'EOF'
version: '3.8'

services:
  # Load Balancer
  nginx-lb:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx-lb.conf:/etc/nginx/nginx.conf
      - ../web/dist:/usr/share/nginx/html
    depends_on:
      - secretly-1
      - secretly-2
      - secretly-3
    restart: unless-stopped

  # Application Instances
  secretly-1:
    build: ../../server
    environment:
      - SECRETLY_ENV=production
      - SECRETLY_DB_URL=postgresql://secretly:${DB_PASSWORD}@postgres:5432/secretly
      - SECRETLY_REDIS_URL=redis://redis:6379
      - SECRETLY_INSTANCE_ID=1
    volumes:
      - app-data-1:/data
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  secretly-2:
    build: ../../server
    environment:
      - SECRETLY_ENV=production
      - SECRETLY_DB_URL=postgresql://secretly:${DB_PASSWORD}@postgres:5432/secretly
      - SECRETLY_REDIS_URL=redis://redis:6379
      - SECRETLY_INSTANCE_ID=2
    volumes:
      - app-data-2:/data
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  secretly-3:
    build: ../../server
    environment:
      - SECRETLY_ENV=production
      - SECRETLY_DB_URL=postgresql://secretly:${DB_PASSWORD}@postgres:5432/secretly
      - SECRETLY_REDIS_URL=redis://redis:6379
      - SECRETLY_INSTANCE_ID=3
    volumes:
      - app-data-3:/data
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # Database with Read Replicas
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=secretly
      - POSTGRES_USER=secretly
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres-master.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  postgres-replica:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=secretly
      - POSTGRES_USER=secretly
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - PGUSER=postgres
    volumes:
      - postgres-replica-data:/var/lib/postgresql/data
    command: |
      bash -c "
      until pg_basebackup --pgdata=/var/lib/postgresql/data -R --slot=replication_slot --host=postgres --port=5432
      do
        echo 'Waiting for primary to connect...'
        sleep 1s
      done
      echo 'Backup done, starting replica...'
      chmod 0700 /var/lib/postgresql/data
      postgres
      "
    depends_on:
      - postgres
    restart: unless-stopped

  # Redis Cluster
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --replica-read-only no
    volumes:
      - redis-data:/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  redis-replica:
    image: redis:7-alpine
    command: redis-server --appendonly yes --replicaof redis 6379
    volumes:
      - redis-replica-data:/data
    depends_on:
      - redis
    restart: unless-stopped

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus-scaled.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-scaled-dashboards:/etc/grafana/provisioning/dashboards
    restart: unless-stopped

volumes:
  app-data-1:
  app-data-2:
  app-data-3:
  postgres-data:
  postgres-replica-data:
  redis-data:
  redis-replica-data:
  prometheus-data:
  grafana-data:

networks:
  default:
    driver: bridge
EOF

# Create load balancer configuration
cat > optimization/scaling/nginx-lb.conf << 'EOF'
events {
    worker_connections 2048;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    keepalive_requests 1000;
    
    # Compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=1r/s;

    # Upstream servers
    upstream secretly_backend {
        least_conn;
        server secretly-1:8080 max_fails=3 fail_timeout=30s;
        server secretly-2:8080 max_fails=3 fail_timeout=30s;
        server secretly-3:8080 max_fails=3 fail_timeout=30s;
        
        # Health checks
        keepalive 32;
    }

    # Caching
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=app_cache:10m max_size=1g inactive=60m use_temp_path=off;

    server {
        listen 80;
        server_name _;

        # Security headers
        add_header X-Frame-Options DENY always;
        add_header X-Content-Type-Options nosniff always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;

        # Static files
        location / {
            root /usr/share/nginx/html;
            try_files $uri $uri/ /index.html;
            
            # Caching for static assets
            location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
                expires 1y;
                add_header Cache-Control "public, immutable";
            }
        }

        # API endpoints with rate limiting
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://secretly_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Caching for GET requests
            proxy_cache app_cache;
            proxy_cache_valid 200 5m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            proxy_cache_lock on;
        }

        # Authentication endpoints with stricter rate limiting
        location /api/auth/ {
            limit_req zone=auth burst=5 nodelay;
            
            proxy_pass http://secretly_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # No caching for auth endpoints
            proxy_no_cache 1;
            proxy_cache_bypass 1;
        }

        # Health check endpoint
        location /health {
            proxy_pass http://secretly_backend;
            access_log off;
        }

        # Swagger documentation
        location /swagger/ {
            proxy_pass http://secretly_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Status page for monitoring
        location /nginx_status {
            stub_status on;
            access_log off;
            allow 127.0.0.1;
            deny all;
        }
    }
}
EOF

# Create auto-scaling script
log_info "Creating auto-scaling capabilities..."
cat > optimization/scaling/auto-scale.sh << 'EOF'
#!/bin/bash

# Auto-scaling script for Secretly
# Monitors system metrics and scales services automatically

# Configuration
MAX_INSTANCES=10
MIN_INSTANCES=2
CPU_THRESHOLD_UP=80
CPU_THRESHOLD_DOWN=30
MEMORY_THRESHOLD_UP=85
MEMORY_THRESHOLD_DOWN=40
SCALE_UP_COOLDOWN=300  # 5 minutes
SCALE_DOWN_COOLDOWN=600  # 10 minutes

# Get current metrics
get_metrics() {
    # Get CPU usage
    CPU_USAGE=$(docker stats --no-stream --format "table {{.CPUPerc}}" | grep -v CPU | sed 's/%//' | awk '{sum+=$1} END {print sum/NR}')
    
    # Get memory usage
    MEMORY_USAGE=$(docker stats --no-stream --format "table {{.MemPerc}}" | grep -v MEM | sed 's/%//' | awk '{sum+=$1} END {print sum/NR}')
    
    # Get current instance count
    CURRENT_INSTANCES=$(docker-compose ps -q secretly-* | wc -l)
    
    echo "Current metrics: CPU=${CPU_USAGE}%, Memory=${MEMORY_USAGE}%, Instances=${CURRENT_INSTANCES}"
}

# Scale up function
scale_up() {
    if [ $CURRENT_INSTANCES -lt $MAX_INSTANCES ]; then
        NEW_INSTANCE=$((CURRENT_INSTANCES + 1))
        echo "Scaling up: Adding secretly-${NEW_INSTANCE}"
        
        # Add new service to docker-compose
        docker-compose up -d --scale secretly=${NEW_INSTANCE}
        
        # Update load balancer configuration
        update_load_balancer
        
        echo "$(date): Scaled up to ${NEW_INSTANCE} instances" >> scaling.log
    else
        echo "Maximum instances reached (${MAX_INSTANCES})"
    fi
}

# Scale down function
scale_down() {
    if [ $CURRENT_INSTANCES -gt $MIN_INSTANCES ]; then
        NEW_INSTANCE=$((CURRENT_INSTANCES - 1))
        echo "Scaling down: Removing secretly-${CURRENT_INSTANCES}"
        
        # Remove instance from docker-compose
        docker-compose up -d --scale secretly=${NEW_INSTANCE}
        
        # Update load balancer configuration
        update_load_balancer
        
        echo "$(date): Scaled down to ${NEW_INSTANCE} instances" >> scaling.log
    else
        echo "Minimum instances reached (${MIN_INSTANCES})"
    fi
}

# Update load balancer configuration
update_load_balancer() {
    # Reload nginx configuration
    docker-compose exec nginx-lb nginx -s reload
    echo "Load balancer configuration updated"
}

# Main monitoring loop
main() {
    echo "Starting auto-scaling monitor..."
    
    while true; do
        get_metrics
        
        # Check if we need to scale up
        if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD_UP" | bc -l) )) || (( $(echo "$MEMORY_USAGE > $MEMORY_THRESHOLD_UP" | bc -l) )); then
            echo "High resource usage detected, scaling up..."
            scale_up
            sleep $SCALE_UP_COOLDOWN
        
        # Check if we can scale down
        elif (( $(echo "$CPU_USAGE < $CPU_THRESHOLD_DOWN" | bc -l) )) && (( $(echo "$MEMORY_USAGE < $MEMORY_THRESHOLD_DOWN" | bc -l) )); then
            echo "Low resource usage detected, considering scale down..."
            scale_down
            sleep $SCALE_DOWN_COOLDOWN
        
        else
            echo "Resource usage within normal range"
            sleep 60
        fi
    done
}

# Run the main function
main
EOF

chmod +x optimization/scaling/auto-scale.sh

# Create performance monitoring dashboard
log_info "Setting up performance monitoring..."
cat > optimization/monitoring/performance-dashboard.json << 'EOF'
{
  "dashboard": {
    "id": null,
    "title": "Secretly Performance Dashboard",
    "tags": ["secretly", "performance"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Response Time (seconds)",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "Requests per second"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "Error rate"
          }
        ]
      },
      {
        "id": 4,
        "title": "Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(database_queries_total[5m])",
            "legendFormat": "Queries per second"
          },
          {
            "expr": "histogram_quantile(0.95, rate(database_query_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile query time"
          }
        ]
      },
      {
        "id": 5,
        "title": "System Resources",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(process_cpu_seconds_total[5m]) * 100",
            "legendFormat": "CPU usage %"
          },
          {
            "expr": "process_resident_memory_bytes / 1024 / 1024",
            "legendFormat": "Memory usage MB"
          }
        ]
      },
      {
        "id": 6,
        "title": "Active Instances",
        "type": "stat",
        "targets": [
          {
            "expr": "count(up{job=\"secretly\"})",
            "legendFormat": "Active instances"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
EOF

# Create caching optimization
log_info "Setting up caching optimization..."
cat > optimization/caching/redis-config.conf << 'EOF'
# Redis Configuration for Secretly Caching

# Memory optimization
maxmemory 512mb
maxmemory-policy allkeys-lru

# Persistence
save 900 1
save 300 10
save 60 10000

# Network
tcp-keepalive 300
timeout 0

# Performance
tcp-backlog 511
databases 16

# Logging
loglevel notice
logfile ""

# Security
requirepass ${REDIS_PASSWORD}

# Clustering (if needed)
# cluster-enabled yes
# cluster-config-file nodes.conf
# cluster-node-timeout 15000

# Modules for advanced features
# loadmodule /usr/lib/redis/modules/redisearch.so
# loadmodule /usr/lib/redis/modules/redisjson.so
EOF

# Create performance testing script
log_info "Creating performance testing tools..."
cat > optimization/performance/load-test.sh << 'EOF'
#!/bin/bash

# Load testing script for Secretly
# Tests system performance under various load conditions

# Configuration
BASE_URL="http://localhost:8080"
CONCURRENT_USERS=50
TEST_DURATION=300  # 5 minutes
RAMP_UP_TIME=60    # 1 minute

echo "🚀 Starting Secretly Load Test"
echo "=============================="

# Test 1: Health check endpoint
echo "Test 1: Health Check Load Test"
if command -v ab &> /dev/null; then
    ab -n 1000 -c 10 ${BASE_URL}/health
else
    echo "Apache Bench (ab) not found, skipping test"
fi

# Test 2: API endpoint load test
echo "Test 2: API Load Test"
if command -v curl &> /dev/null; then
    echo "Running concurrent API requests..."
    for i in $(seq 1 $CONCURRENT_USERS); do
        (
            for j in $(seq 1 10); do
                curl -s ${BASE_URL}/api/v1/health > /dev/null
                sleep 0.1
            done
        ) &
    done
    wait
    echo "Concurrent API test completed"
fi

# Test 3: Memory usage test
echo "Test 3: Memory Usage Test"
if command -v docker &> /dev/null; then
    echo "Monitoring memory usage during load..."
    docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"
fi

# Test 4: Database performance test
echo "Test 4: Database Performance Test"
if [ -f "./secretly" ]; then
    echo "Testing database operations..."
    time ./secretly secret create "load-test-$(date +%s)" "test-value" --config secretly-simple.yaml
    time ./secretly secret list --config secretly-simple.yaml > /dev/null
fi

echo "Load testing completed!"
EOF

chmod +x optimization/performance/load-test.sh

# Create optimization summary
log_success "Optimization and scaling setup completed!"

cat << 'EOF'

📈 Optimization and Scaling Summary
===================================

✅ Performance optimization configuration created
✅ Horizontal scaling setup with load balancing
✅ Auto-scaling capabilities implemented
✅ Performance monitoring dashboard configured
✅ Caching optimization with Redis
✅ Load testing tools prepared

📁 Optimization Files Created:
├── optimization/performance/performance-config.yaml
├── optimization/performance/load-test.sh
├── optimization/scaling/docker-compose.scaled.yml
├── optimization/scaling/nginx-lb.conf
├── optimization/scaling/auto-scale.sh
├── optimization/monitoring/performance-dashboard.json
└── optimization/caching/redis-config.conf

🚀 Scaling Features:
- Load balancer with 3 application instances
- Database read replicas for improved performance
- Redis caching with replication
- Auto-scaling based on CPU and memory metrics
- Performance monitoring with Grafana dashboards

⚡ Performance Optimizations:
- Database connection pooling and query optimization
- Application-level caching with Redis
- Static asset compression and CDN support
- Rate limiting and DDoS protection
- Hardware-accelerated encryption

📊 Monitoring and Metrics:
- Real-time performance dashboards
- Auto-scaling based on system metrics
- Load testing tools for performance validation
- Comprehensive system resource monitoring

🔧 Deployment Commands:
# Deploy scaled environment:
cd optimization/scaling && docker-compose -f docker-compose.scaled.yml up -d

# Start auto-scaling:
./optimization/scaling/auto-scale.sh

# Run load tests:
./optimization/performance/load-test.sh

EOF

log_success "Task 10: Optimization and Scaling - COMPLETED!"